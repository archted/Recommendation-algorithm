## 近似最近邻查找





### 近似最近邻查找ANN

NN与ANN

1.NN，Nearest Neighbor Search，最近邻查找问题

比如：KNN，K-Nearest Neighbor，k最近邻，查找离目标数据最近的前k个数据项

2.**ANN，Approximate Nearest Neighbor，近似最近邻检索，在牺牲可接受范围内的精度的情况下提高检索效率**

最近邻检索是线性复杂度的，当处理大规模数据时可以采用ANN方法

LSH，局部敏感哈希是ANN的一种





### MinHash

MinHash是一种降维技术，将一个大的集合中的元素转换为短小的签名，同时保持了这些集合中的元素的相似性,有$P(MinHash(S1)=MinHash(S_2))=Jac(S_1,S_2 )$成立。

#### Hash

主要的**索引技术**：

基于树的索引技术（二叉树，B-Tree,B+Tree）

基于哈希的索引技术，海量数据一般采用哈希的检索方式

基于词的倒排索引

#### MinHash与Jaccard相似度



1. Jaccard相似度

K-Shingles，K-Gram，文档中任意长度为k的字符串,将每篇文档可以表示成文档中出现一次或者多次的k-shingle的集合,完成对文档的切割。比如document="abcdabd"，当2-shingle组成的集合为 {ab,bc,cd,da,bd}。如果两个文档相似，那么他们会有很多的shingles也是相同的。

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210721072603637.png?raw=true" alt="image-20210721072603637" style="zoom:50%;" />

文档相似度计算：

Jaccard相似度$SIM(C_i,C_j)=|C_i∩C_j |/|C_i∪C_j | $

比如计算上图文档1和文档2的相似度：$SIM(C_1,C_2)=2/5$

海量数据，高维 => 矩阵非常大的时候，目标需要找到一个Hash函数，将原来的Jaccard相似度计算，等同于降维后的相似度矩阵计算（Input Matrix => Signature Matrix）

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210721072603637.png?raw=true" alt="image-20210721072603637" style="zoom:50%;" />

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210721074758871.png?raw=true" alt="image-20210721074758871" style="zoom:50%;" />

​                              Signature Matrix

2. MinHash

$P(MinHash(S1)=MinHash(S_2))=Jac(S_1,S_2 )$

=>用签名矩阵Ci，Cj的MinHash值，对他们的Jaccard相似度进行估计

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210721073948525.png?raw=true" alt="image-20210721073948525" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210721073956470.png" alt="image-20210721073956470" style="zoom:100%;" />

#### MinHash的计算

做法：打擂法，有多个hash函数,通过hash i得到最新的行号，如果此时列上的元素为1，而且新行号比原来记录的M值小，那么更新M值 => 通过m个针对row index的Hash函数，完成m次行向量的置换（解决了行向量置换的问题）

### MinHash+LSH

#### LSH算法

LSH，Locality-Sensitive Hashing，局部敏感哈希，是一种近似最近邻查找方法（ANN），在海量数据中找到一个高维度点相似的点集合。

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210721071219303.png?raw=true" alt="image-20210721071219303" style="zoom: 80%;" />

因此，LSH两个作用：

1).检索数据

2).将相似的数据放到同一个Bucket中，这样就可以很方便的进行相邻元素的查找。

#### MinHash+LSH

MinHash解决了Ci和Cj两两之间相似度的计算问题，但是当数据量大的时候，两两之间相似度计算次数为$C_N^2$,当数据量N很大（>100万），计算量非常大。因此采用LSH的做法，即通过分桶的方式加速查找相似元素。

做法：

将Signiture矩阵分成b组，每组由r行组成。对每一组进行hash，各个组设置不同的桶空间。如果在某个band上两列值是相似的，那么就认为这两列signiture是相似的。

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210721082424362.png?raw=true" alt="image-20210721082424362" style="zoom:80%;" />

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210721082443010.png" alt="image-20210721082443010" style="zoom:100%;" />

* 两列签名值相似性与被分在同一个桶中的关系

假设**对于某行，两列签名值相同的概率为s**（两列的相似度）

在某个band，值都相等的概率是$s^r$

在某个band，值不相同的概率是$1-s^r$

两个文档存在b个band，这b个band都不相同的概率是$(1-s^r )^b$

b个band里，至少有一个相同的概率是$1-(1-s^r )^b$

=> 两列成为候选相似对的概率是$1-(1-s^r )^b$

称之为**And then OR方法**，先要求每个band的所有对应元素必须都相同，再要求多个band中至少有一个相同。符合这两条，才能发生hash碰撞。实现了局部相似=>全局相似的目的。

假设s=0.8，20个band，每个band 5行，即b=20, r=5

在某个band，值都相等的概率是$0.8^5=0.328$

在某个band，值不相同的概率是 $1-0.8^5=0.672$

b个band都不相同的概率是$0.672^20=0.00035$

b个band里，至少有一个相同的概率是$1-0.672^20=0.9996$

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722062324734.png" alt="image-20210722062324734" style="zoom:100%;" />

<center>b=20, r=5时的概率表</center>

当b=100,r=4时，$(1-s_{}^4 )^{100}$   的曲线

<img src="https://github.com/archted/markdown-img/blob/main/img/image-20210722063018812.png?raw=true" alt="image-20210722063018812" style="zoom: 80%;" />

注意：横坐标表示**对于一行，两列签名值([1,1]的向量)相同的概率，即s**，纵坐标表示这两列签名值([b*r,1]的向量)被分到同一个桶里面的概率。

当s超过某个阈值后，两个用户成为candidate用户的概率会迅速增加并接近于1。这个阈值，也就是概率变化最陡的地方，近似为$t=(1/b)^(1/r)$。

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722063053617.png" alt="image-20210722063053617" style="zoom:100%;" />



* 参数定义

在使用过程中，我们需要确定

Smin，相似用户的阈值定义（近邻定义）

Signature向量的长度，降到k维embedding

而分桶时，因为中间概率变化最陡，大概率也会分到同一个桶里面。如果概率变化最陡的地方小于Smin，则这些数据不会被分到同一个 桶里，反之表示这些数据非常相似，会被分到同一个桶里面。



如果想要尽可能少的出现false negative，需要选择b和r使得概率变化最陡的地方小于Smin（比如s在0.5以上才属于相似用户，选择b和r使得S曲线的最陡处小于0.5）;如果想要保证计算速度较快，并且尽可能少出现false positive，那么最好选择b和r使得概率变化最陡的地方较大（比如b=20，r=6）这样，s较小的两个用户就很难成为candidate用户，但同时也会有一些“潜在”的相似用户不会被划分到同一个桶内

* LSH的一般定义

**解释：两个对象，距离越近相似的概率会大于d1，距离越远相似的概率会小于d2；**



LSH的一般定义：

Locality-Sensitive Hashing是满足一定条件的Hash函数簇

令d1<d2是定义在距离测定d下得两个距离值，如果一个函数族的每一个函数f满足：

如果d(x,y)<=d1,则f(x)=f(y)的概率至少为p1，即P(f(x)=f(y)) >= p1

如果d(x,y)>=d2,则f(x)=f(y)的概率至多为p2，即p(f(x)=f(y)) <= p2

那么称F为(d1,d2,p1,p2)-sensitive的函数族。

Jaccard相似性对应的LSH为MinHash,是(d1,d2,1-d1,1-d2)-sensitive

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722063555454.png" alt="image-20210722063555454" style="zoom:100%;" />



* 总结

总结：数据不仅海量，而且高维 => MinHash降维，采用LSH分桶的方式查找相似数据。



#### 工具：datasketch

用户手册：http://ekzhu.com/datasketch/lsh.html

datasketch，用于海量数据相似查找，是一种Python工具

支持多种统计方式

对常见的统计需求，在精确度，存储成本，计算成本之间进行了折衷，对每一个计算元素只接触一次的情况下，得到精度相当高的统计结果

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722070939085.png" alt="image-20210722070939085" style="zoom:100%;" />

##### Data Sketch包

1. MinHash()

参数：

num_perm参数，Hash置换函数设定个数，默认为128，如果需要提高精度，可以提高该数值，比如设置num_perm=256

API函数：

update函数，内容Hash化m1.update(content)

merge函数，Hash合并，比如m1.merge(m2)

jaccard(),使用MinHash预估的Jaccard相似度, m1.jaccard(m2)

```python
from datasketch import MinHash
data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']
data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']

m1 = MinHash()
m2 = MinHash()
for d in data1:
	m1.update(d.encode('utf8'))
for d in data2:
	m2.update(d.encode('utf8'))
print("使用MinHash预估的Jaccard相似度", m1.jaccard(m2))
s1 = set(data1)
s2 = set(data2)
actual_jaccard = loat(len(s1.intersection(s2)))/float(len(s1.union(s2)))
print("Jaccard相似度实际值", actual_jaccard)
# 使用MinHash预估的Jaccard相似度 0.6015625,Jaccard相似度实际值 0.625
```

##### Index包

2. MinHashLSH()

用户手册：http://ekzhu.com/datasketch/lsh.html

threshold 参数，Jaccard 距离阈值设定，默认为0.9

num_perm参数，Hash置换函数设定个数，默认为128

weights (tuple, optional)，优化Jaccard 阈值，能够弹性选择

params (tuple, optional)，bands 的数量与规模大小

insert(key)，内容载入LSH系统

remove(key)，移除相关hash值

query(key)，查询内容需要时minHash化

```python
from datasketch import MinHash, MinHashLSH
data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']
data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']
data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']

m1 = MinHash()
m2 = MinHash()
m3 = MinHash()
for d in data1:
	m1.update(d.encode('utf8'))
for d in data2:
	m2.update(d.encode('utf8'))
for d in data3:
	m3.update(d.encode('utf8'))

lsh = MinHashLSH(threshold=0.5, num_perm=128)
lsh.insert("m2", m2)
lsh.insert("m3", m3)
result = lsh.query(m1)
print("近似的邻居（Jaccard相似度>0.5）", result)
#近似的邻居（Jaccard相似度>0.5） ['m2', 'm3']

```

3. MinHashLSHForest()

局部敏感随机投影森林

http://ekzhu.com/datasketch/lshforest.html

论文：http://ilpubs.stanford.edu:8090/678/1/2005-14.pdf

求近似最近邻方法的一种（ANN）

随机选取一个从原点出发的向量，与这个向量垂直的直线将平面内的点划分为了两部分。当数目比较大的时候，可以继续进行划分

对应于一棵深度为2，有4个叶节点的树（划分出4个部分）。一直划分，直到每个叶节点中点的数目都达到一个足够小的数目，也就是将每次搜索与计算的点的数目减小到一个可接受的范围。建立多个随机投影树构成随机投影森林，将森林的综合结果作为最终的结果



点积大于零的点划分到左子树，点积小于零的点划分到右子树

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722072040758.png" alt="image-20210722072040758" style="zoom:100%;" />

在上一次划分的基础上继续划分，再次随机选取一个向量

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722072058919.png" alt="image-20210722072058919" style="zoom:100%;" />



num_perm参数，Hash置换函数设定个数，默认为128

l 参数，代表prefix trees的数量，默认为8

index()  在检索前，需要使用index,相当于建树的过程

add(key)，内容载入LSHForest系统

query(key, k)，查询与key相似的Top-K个邻居

```python
from datasketch import MinHash, MinHashLSH, MinHashLSHForest
data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']
data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']
data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']
# 创建MinHash对象
m1 = MinHash()
m2 = MinHash()
m3 = MinHash()
for d in data1:
	m1.update(d.encode('utf8'))
for d in data2:
	m2.update(d.encode('utf8'))
for d in data3:
	m3.update(d.encode('utf8'))
# 创建LSH Forest
forest = MinHashLSHForest()
forest.add("m2", m2)
forest.add("m3", m3)
# 在检索前，需要使用index
forest.index()
# 判断forest是否存在m2, m3
print("m2" in forest)
print("m3" in forest)
# 查询forest中与m1相似的Top-K个邻居
result = forest.query(m1, 2)
print("Top 2 邻居", result)

/*
True
True
Top 2 邻居 ['m2', 'm3']
*/
```

4. MinHashLSHEnsemble()

http://ekzhu.com/datasketch/lshensemble.html

论文：http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf

MinHashLSHEnsemble是对于相似度的另一种计算方式：

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722072952081.png" alt="image-20210722072952081" style="zoom:100%;" />

对于X和X'，Jaccard计算差很大，因为$Jaccard=|Q∩X|/|Q∪X|$,会对大的集合进行惩罚，因此对右边set计算得到的相似度会比较小；而MinHashLSHEnsemble计算相似度的公式为$Containment=|Q∩X|/|Q|$,对上图中左右两边的相似度计算得到的是相同的数值。

threshold 参数，Jaccard 距离阈值设定，默认为0.9

num_perm参数，Hash置换函数设定个数，默认为128

index()，内容载入LSHEnsemble系统

query(key, size)，查询与key相似的邻居

```python
from datasketch import MinHash, MinHashLSH, MinHashLSHEnsemble
data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']
data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']
data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']
# 创建MinHash对象
m1 = MinHash()
m2 = MinHash()
m3 = MinHash()
for d in data1:
	m1.update(d.encode('utf8'))
for d in data2:
	m2.update(d.encode('utf8'))
for d in data3:
	m3.update(d.encode('utf8'))
# 创建LSH Ensemble
lshensemble = MinHashLSHEnsemble(threshold=0.8, num_perm=128)
# Index takes an iterable of (key, minhash, size)
lshensemble.index([("m2", m2, len(data2)), ("m3", m3, len(data3))])
# 判断lshensemble是否存在m2, m3
print("m2" in lshensemble)
print("m3" in lshensemble)
# 查询与m1相似度Containment大于0.8的集合
print("与m1相似度大于0.8的集合：")
for key in lshensemble.query(m1, len(data1)):
    print(key)

/*
True
True
与m1相似度大于0.8的集合：
m2
m3

*/
```

#### 使用MinHashLSH对文本进行近似近邻查找

Step1，对文档进行k-shingle，即将文档切割成一个一个的元素，这些元素是由很多的字符串组成的（由k个字符串组成）

Step2，使用MinHash得到集合元素的签名

Step3，使用LSH加快候选相似对的查找，得到可能的候选对

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722074754322.png" alt="image-20210722074754322" style="zoom:100%;" />

下面以天猫双11新闻为例，演示MinHashLSH方法。

```python
# 读取文件
f = open('./sentences.txt', 'r', encoding='UTF-8')
text = f.read()
# 以句号，叹号，问号作为分隔，去掉\n换行符号
sentences = re.split('[。！？]', text.replace('\n', ''))
# 最后一行如果为空，则删除
if sentences[len(sentences)-1] == '':
    sentences.pop()
# 将item_text进行分词
def get_item_str(item_text):
# 对item_str创建MinHash
def get_minhash(item_str):
# 设置停用词
stop = [line.strip().decode('utf-8') for line in open('stopword.txt').readlines()]
# 得到分词后的documents
documents = []
for item_text in sentences:
    # 将item_text进行分词
    item_str = get_item_str(item_text)
    documents.append(item_str)
# 创建LSH Forest及MinHash对象
minhash_list = []
forest = MinHashLSHForest()
for i in range(len(documents)):
    #得到train_documents[i]的MinHash
    temp = get_minhash(documents[i])
    minhash_list.append(temp)
    forest.add(i, temp)
# index所有key，以便可以进行检索
forest.index()
query = '00:01:36，2019天猫双11总成交额超100亿元'
# 将item_text进行分词
item_str = get_item_str(query)
# 得到item_str的MinHash
minhash_query = get_minhash(item_str)
# 查询forest中与m1相似的Top-K个邻居
result = forest.query(minhash_query, 3)
for i in range(len(result)):
    print(result[i], minhash_query.jaccard(minhash_list[result[i]]), documents[result[i]].replace(' ', ''))
print("Top 3 邻居", result)
```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722075120060.png" alt="image-20210722075120060" style="zoom:100%;" />

### SimHash

#### 汉明距离

汉明（Hamming）距离：

两个二进制串中不同位的数量

10**001**001

10**110**001

比如10001001和10110001，有3位不同，因此汉明距离=3

向量相似度越高，对应的汉明距离越小

在图片的识别中，汉明距离在0-10之间认为是相似的，采用汉明距离计算相似度可以大幅提升效率

#### 算法流程

1. 简介

LSH局部敏感哈希的一种

Paper: similarity estimation techniques from rounding algorithms

Google采用SimHash进行网页查重

Detecting Near-Duplicates for Web Crawling

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.7794&rep=rep1&type=pdf

2. 通过SimHash算法得到每篇文章指纹的步骤

Step1，设置SimHash的位数，比如32位，需要综合考虑存储成本以及数据集的大小

Step2，初始化SimHash，将各位初始化为0 

Step3，提取文本中的特征，比如采用2-Shingles

"the cat sat on the mat"=>{"th", "he", "e ", " c", "ca", "at", "t ", " s", "sa", " o", "on", "n ", " t", " m", "ma"} 

Step4，使用传统的hash函数计算各个word的hashcode

比如："th".hash = -502157718 ，"he".hash = -369049682，…… 

Step5，对各word的hashcode的每一位，如果该位为1，则simhash相应位的值加它的权重（通常是出现的频率）；否则减它的权重 

Step6，计算最后得到的32位的SimHash，如果该位大于1，则设为1；否则设为0 

3. 计算两篇文章的相似度

Step1，通过SimHash算法得到每篇文档的指纹（fingerprint）

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722080048185.png" alt="image-20210722080048185" style="zoom:100%;" />

Step2，计算两个文档指纹的海明距离

通常2篇文档的Hamming距离在3以内，就认为相似度比较高 => 两篇文档基本相同

4. 如何通过SimHash进行相似文档的检索

假设我们有10亿文档(2^34)，每来一个新文本我们都需要做10亿次的汉明距离计算=>计算量太大

通过抽屉原理，如果K=3，那么可以将SimHash分成 K+1=4段，如果两个SimHash的Hamming距离为3，那么至少有一段（16位）的SimHash是相同的

采用索引的方式进行查找加速，取出每一段相同的候选文本

文档库中有2^34 个签名，那么匹配上每个段的结果最多有2^(34-16)=262144个候选结果，四个段返回的结果总数=4*262144（大概100万）

原本需要比较10亿次 => 现在只需要比较100万次了，即在100万个候选结果中进行比较

* 具体过程：

如果SimHash有64位，Hamming距离<=3认为相似，相似的SimHash有C(64,3)种可能，计算量还是太大了。

因此采用下面的方法：

通过抽屉原理，如果K=3，那么可以将SimHash分成 K+1=4段

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210722081108433.png" alt="image-20210722081108433" style="zoom:100%;" />

分成4段，分别查找，只要有一段匹配就是候选

采用数据库索引的方式进行匹配查找效率高

#### 在python中使用

pip install git+https://github.com/leonsim/simhash

Python实现的SimHash，可以得到某个文本的SimHash，以及SimHash距离

对3个文本进行SimHash，并计算文本SimHash距离

```python
data = [
    '这个程序代码太乱,那个代码规范',
    '这个程序代码不规范,那个更规范',
    '我是佩奇，这是我的弟弟乔治'
]
vec = TfidfVectorizer()
D = vec.fit_transform(data)
voc = dict((i, w) for w, i in vec.vocabulary_.items())
# 生成Simhash
sh_list = []
for i in range(D.shape[0]):
    Di = D.getrow(i)
    # features表示 (token, weight)元祖形式的列表
    features = zip([voc[j] for j in Di.indices], Di.data)
    sh_list.append(Simhash(features))
print(sh_list[0].distance(sh_list[1]))
print(sh_list[0].distance(sh_list[2]))
print(sh_list[1].distance(sh_list[2]))

/*
23
20
25

*/
```

但是从结果可以看到0 1 2的区分度并不高，实际上打印出的第一项应该是要比较小才对，产生这个现象是因为没有对data进行分词，那么TfidfVectorizer就会将`这个程序代码太乱`当成一个单词，将`那个代码规范`当成一个单词，将`这个程序代码不规范`、`那个更规范`、`我是佩奇`、`这是我的弟弟乔治`分别当做一个单词。

因此，做词法分析的话第一步一定要先分词，然后用Tfidf提取权重。

**中文不比英文，词语之间有着空格的自然分割，所以我们首先要进行分词处理，可以使用中文分词库jieba进行分词。**

```python
import jieba
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
text = """我是一条天狗呀！
我把月来吞了，
我把日来吞了，
我把一切的星球来吞了，
我把全宇宙来吞了。
我便是我了！"""
sentences = text.split() #['我是一条天狗呀！', '我把月来吞了，', '我把日来吞了，', '我把一切的星球来吞了，', '我把全宇宙来吞了。', '我便是我了！']
sent_words = [list(jieba.cut(sent0)) for sent0 in sentences] #[['我', '是', '一条', '天狗', '呀', '！'], ['我', '把', '月', '来', '吞', '了', '，'], ['我', '把', '日来', '吞', '了', '，'], ['我', '把', '一切', '的', '星球', '来', '吞', '了', '，'], ['我', '把', '全宇宙', '来', '吞', '了', '。'], ['我', '便是', '我', '了', '！']]
document = [" ".join(sent0) for sent0 in sent_words]
print(document)
# ['我 是 一条 天狗 呀 ！', '我 把 月 来 吞 了 ，', '我 把 日来 吞 了 ，', '我 把 一切 的 星球 来 吞 了 ，', '我 把 全宇宙 来 吞 了 。', '我 便是 我 了 ！']

tfidf_model = TfidfVectorizer().fit(document)
print(tfidf_model.vocabulary_)
# {'一条': 1, '天狗': 4, '日来': 5, '一切': 0, '星球': 6, '全宇宙': 3, '便是': 2}
sparse_result = tfidf_model.transform(document)
print(sparse_result)
# (0, 4)	0.707106781187
# (0, 1)	0.707106781187
# (2, 5)	1.0
# (3, 6)	0.707106781187
# (3, 0)	0.707106781187
# (4, 3)	1.0
# (5, 2)	1.0
```

下面采用手动分词后，，然后用Tfidf提取词向量权重。

```python
from sklearn.feature_extraction.text import TfidfVectorizer

data = [
    '这个 程序 代码 太乱 那个 代码 规范',
    '这个 程序 代码 不 规范 那个 更 规范',
    '我 是 佩奇 这 是 我的 弟弟 乔治'
]
vec = TfidfVectorizer()
D = vec.fit_transform(data)
voc = dict((i, w) for w, i in vec.vocabulary_.items())
# 生成Simhash
sh_list = []
for i in range(D.shape[0]):
    Di = D.getrow(i)
    # features表示 (token, weight)元祖形式的列表
    features = zip([voc[j] for j in Di.indices], Di.data)
    sh_list.append(Simhash(features))
print(sh_list[0].distance(sh_list[1]))
print(sh_list[0].distance(sh_list[2]))
print(sh_list[1].distance(sh_list[2]))

/*
16
32
38

*/
```

### 总结

近似最近邻查找ANN，是在空间时间有限的条件下对最近邻的一种近似求解

LSH是一种ANN，在海量数据中找到一个高维度点相似的点集合

K-Shingles，K-Gram，对文档的转换，也就是对文档进行切割

MinHash降维，LSH减少查找范围

MinHash是一种降维技术，将一个大的集合中的元素转换为短小的签名，同时保持了这些集合中的元素的相似性,有$P(MinHash(S1)=MinHash(S_2))=Jac(S_1,S_2 )$成立。

MinHash适用于集合表示，或者0-1数据类型（one-hot词向量）的降维与近邻查找

MinHashLSH 关注的是签名对可能的相似性,采用分桶的方式，将相似的集合至少在一个区间内冲撞，将冲撞的集合作为候选对象进行比较，从而找出相似的对象，时间复杂度由$O(n^2 )=>O(n)$

SimHash算法高效，适用于长文本，Google将SimHash运用到了网页的去重中

