# 基于隐变量的推荐模型

## 矩阵分解

上一篇介绍了协同过滤，其重点就是在人-物品矩阵上，其中心思想是去对人或者物品进行聚类，从而找到相似人或者相似物品，用群体的智慧为个人进行推荐，但是，这种近邻模型也存在好多问题：

1. 随着人和物品的增加，系统的推荐效果并不是线性增加的
2. 矩阵中元素稀疏，在计算过程中，随机的增减一个维度，对结果影响很大

为了解决上面的问题，于是就有人发明了矩阵分解的方法，矩阵分解简单讲看下面图：
![img](http://static.zybuluo.com/zhuanxu/fslxoayu8rgrv051dk86hk8p/image_1canl1ojh1ab1udbkasoho16jh9.png)
假设用户物品的评分矩阵 A 是 m 乘以 n 维，即一共有 m 个用户，n 个物品。我们选一个相对 m 和 n 很小的数 k，通过一套算法得到两个矩阵 U 和 V，矩阵 U 的维度是 m 乘以 k，矩阵 V 的维度是 n 乘以 k。

我们现在对上面的整个过程进行解释。

刚开始我们有用户-物品的矩阵，但是呢，整个矩阵中元素非常稀疏，也就是说我们所能得到的有效信息非常好，现在我们希望通过一定的方法来**补全矩阵**。

补全的方法呢就是模拟矩阵中元素的生成过程，此处我们假设矩阵中i,j位置处的元素是由一个用户向量$$u_i$$和物品向量$$item_j$$相乘得到，此处 $$u_i$$和$$item_j$$有相同的维度， 我们称作用户和物品的**隐向量**，用数学描述就是：$$r_{i,j} = u_i * item_j$$

## 矩阵分解求解方法

现在我们知道了分解原理，下一步就是如何去求解了，介绍两种方法，一种是梯度下降，另一种是交替最小二乘法 ALS。

我们先来看“梯度下降”（Gradient Descent）的方法，要用梯度下降来优化的话，我们需要先定义损失函数：

$$\underset{u_i,i_j}{min}\sum_{i,j}{(r_{ij}-u_iI_j^T)^2+\lambda (\left \| u_i \right \|^2+\left \| I_j \right \|^2)}$$

接着我们来看交替最小二乘法 ALS，其原理是：先假设user矩阵的特征值，通过梯度下降求解item的特征值；再假定item的特征值，求解user的特征值，

上面我们对于用户的评分只建模了用户和物品的隐向量，但是实际中有一些用户会给出偏高的评分；有一些物品也会收到偏高的评分，甚至整个平台所有的物品的评分都会有个偏置，基于此，我们修正下我们的损失函数：

$$\underset{u_i,i_j}{min}\sum_{i,j}{(r_{ij}-u_iI_j^T-\mu - b_i - b_j)^2+\lambda (\left \| u_i \right \|^2+\left \| I_j \right \|^2 + b_i^2 + b_j^2)}$$

在**加入了偏置信息**的基础上，我们在加入**引入用户的一些隐性行为**，将这种隐反馈考虑进来，特别适合一直浏览，但是不怎么进行评价的用户。此时我们建模得到下面的式子：
![img](http://static.zybuluo.com/zhuanxu/tao81og78ft3heobp7wtptns/image_1c1k5f7jk1143ev1r261s84bp637.png)其中R(u)是用户u有隐性行为的item集合，y则是对item隐性行为的向量建模，如果用户有多个隐性行为，我们同样可以再加上一个隐向量：
![img](http://static.zybuluo.com/zhuanxu/lrdroej7vkakx4gz261q7a3q/image_1c1k5isn2p5d3d81qf215aa1q4h3k.png)

更多的关于svd相关的，可以参考我之前的文章[推荐系统算法之surprise实战](https://www.zybuluo.com/zhuanxu/note/987561)，理解结合了代码，分析了不同的算法实现。

## 负采样

下面我们来讨论一个关于隐式反馈的问题，我们以浏览举例子，我们在收集用户浏览数据的时候，一般只有用户明确浏览了哪个物品的记录，一般没有用户明确不浏览哪个文档的记录，这就导致我们的训练样本中数据只有1个类别，因此我们需要一些负采样的手段，可行的方法有两个：

- 随机均匀采样，和正样本保持 1：1
- 按照物品热点采样，理论上这个物品很热门，用户都没有看，这个用户更加有可能是对这个物品真不感兴趣

现在我们已经通过负采样的方式得到了负样本了，下一步是我们在正样本的基础上加入置信度，这个**置信度**是通过统计用户浏览物品的次数得到，用户如果反复浏览，说明用户对这个物品就越感兴趣，所以我们假设

1. 用户没有浏览，分数为0（负采样得到）
2. 浏览一次，得分为1，随着浏览次数增加，得分提高

此时我们的损失函数为：

$$\underset{u_i,i_j}{min}\sum_{i,j}{c_{ij}(r_{ij}-u_iI_j^T)^2+\lambda (\left \| u_i \right \|^2+\left \| I_j \right \|^2)}$$

其中 ，$$c_{ij} = 1+\alpha C$$是置信度，表示如果浏览次数越高，表示用户越感兴趣，其权重就越高。

现在假设我们已经计算出用户和物品的隐向量了，接下去我们就要去计算用户对所有物品的评分了，从中选择topk的做推荐，这在工程上就会面临一个计算量问题，在上一篇文章[深入浅出推荐系统之简单推荐模型](https://www.zybuluo.com/zhuanxu/note/1104086)中，讨论协同过滤的时候就有讲过如何进行计算的问题，可以去查看下，其主要思想就是将计算用户对所有物品的评分拆分成了MapReduce任务，非常精妙，其大致原理如下：
![img](http://static.zybuluo.com/zhuanxu/bh5newqjudgqvfoks48y1y02/image_1cao38p06o1mbqb1rlhibuief9.png)

现在总结下上面讲的隐向量模型，隐向量模型尝试建立从隐藏变量到最终预测值之间的关系，在前面介绍的矩阵分解中，我们的输入是用户id和物品id，然后通过矩阵分解的方法，我们得到了用户的隐藏向量和物品的隐藏向量。

## 分解机

上面这种方法的问题是：我们无法对用户和物品显性特征的建模，譬如我们已经得到了用户的用户画像，或者物品的物品画像，但是我们不能融合进入我们的模型，我们如果要对这些显性特征进行建模的话，**一个可行的方案就是逻辑回归，于是有人就对矩阵分解和逻辑回归进行了结合，提出了"分解机"的模型.**

[基于隐变量的推荐模型 - 作业部落 Cmd Markdown 编辑阅读器 (zybuluo.com)](https://www.zybuluo.com/zhuanxu/note/1107003)

