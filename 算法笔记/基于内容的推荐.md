## 基于内容的推荐

### 基于内容的推荐总览

•依赖性低，不需要动态的用户行为，只要有内容就可以进行推荐

•系统不同阶段都可以应用

系统冷启动，内容是任何系统天生的属性，可以从中挖掘到特征，实现推荐系统的冷启动。

商品冷启动，不论什么阶段，总会有新的物品加入，这时只要有内容信息，就可以帮它进行推荐

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726093220777.png" alt="image-20210726093220777" style="zoom:100%;" />

步骤：

1.物品表示 Item Representation：

为每个item抽取出features

2.用户的喜好特征学习Profile Learning：

利用一个用户过去喜欢（不喜欢）的item的特征数据，来学习该用户的喜好特征（profile）；

3.生成推荐列表Recommendation Generation：

通过用户profile与候选item的特征，推荐相关性最大的item。

### 什么是N-Gram

分词方法分为两类：word或者char，即定义特征为词（word）或n-gram字符

**N-Gram字符（N元语法）是一种分词方法，可以理解为滑动窗口**

•N=1时为unigram，N=2为bigram，N=3为trigram

比如文本：A B C D E，对应的Bi-Gram为A B, B C, C D, D E

•当一阶特征不够用时，可以用N-Gram做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征，采用N-Gram => 可以理解是相邻两个关键词的特征组合

### 余弦相似度计算

余弦相似度：

•通过测量两个向量的夹角的余弦值来度量它们之间的相似性。

•判断两个向量⼤致方向是否相同，方向相同时，余弦相似度为1；两个向量夹角为90°时，余弦相似度的值为0，方向完全相反时，余弦相似度的值为-1。

•两个向量之间夹角的余弦值为[-1, 1]

给定属性向量A和B，A和B之间的夹角θ余弦值可以通过点积和向量长度计算得出

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726093757104.png" alt="image-20210726093757104" style="zoom:100%;" />

计算A和B的余弦相似度：

•句子A：这个程序代码太乱，那个代码规范

•句子B：这个程序代码不规范，那个更规范

•Step1，分词 ，即特征提取

句子A：这个/程序/代码/太乱，那个/代码/规范

句子B：这个/程序/代码/不/规范，那个/更/规范

•Step2，列出所有的词

这个，程序，代码，太乱，那个，规范，不，更

•Step3，计算词频

句子A：这个1，程序1，代码2，太乱1，那个1，规范1，不0，更0

句子B：这个1，程序1，代码1，太乱0，那个1，规范2，不1，更1

计算A和B的余弦相似度：

•Step4，计算词频向量的余弦相似度

句子A：（1，1，2，1，1，1，0，0）

句子B：（1，1，1，0，1，2，1，1）

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726093924311.png" alt="image-20210726093924311" style="zoom:100%;" />

结果接近1，说明句子A与句子B是相似的

### 为酒店建立内容推荐系统

西雅图酒店数据集：

下载地址：https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv

字段：name, address, desc

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726093604598.png" alt="image-20210726093604598" style="zoom:100%;" />

基于用户选择的酒店，推荐相似度高的Top10个其他酒店



**方法：计算当前酒店特征向量与整个酒店特征矩阵的余弦相似度，取相似度最大的Top-k个**



下面说明如何利用`TfidfVectorizer(ngram_range=(1, 3))`进行Top-k的内容推荐：

•Step1，对酒店描述（Desc）进行特征提取

•N-Gram，提取N个连续字的集合，作为特征

•TF-IDF，按照(min_df, max_df)提取关键词，并生成TFIDF矩阵

•Step2，计算酒店之间的相似度矩阵

•余弦相似度

•Step3，**对于指定的酒店，选择相似度最大的Top-K个酒店进行输出**

```python
def clean_text(text):
    # 全部小写
    text = text.lower()
    ……
    return text
df['desc_clean'] = df['desc'].apply(clean_text)
# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print(tfidf_matrix)
print(tfidf_matrix.shape)
# 计算酒店之间的余弦相似度（线性核函数）
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
print(cosine_similarities)
```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726095257725.png" alt="image-20210726095257725" style="zoom:100%;" />

152家酒店，之间的相似度矩阵（1-Gram, 2-Gram, 3-Gram)

```python
# 基于相似度矩阵和指定的酒店name，推荐TOP10酒店
def recommendations(name, cosine_similarities = cosine_similarities):
    recommended_hotels = []
    # 找到想要查询酒店名称的idx
    idx = indices[indices == name].index[0]
    print('idx=', idx)
    # 对于idx酒店的余弦相似度向量按照从大到小进行排序
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)
    # 取相似度最大的前10个（除了自己以外）
    top_10_indexes = list(score_series.iloc[1:11].index)
    # 放到推荐列表中
    for i in top_10_indexes:
        recommended_hotels.append(list(df.index)[i])
    return recommended_hotels
print(recommendations('Hilton Seattle Airport & Conference Center'))
print(recommendations('The Bacon Mansion Bed and Breakfast'))

```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726095349910.png" alt="image-20210726095349910" style="zoom:100%;" />

### 总结

基于内容的推荐：

•将你看的item，相似的item推荐给你

•通过物品表示Item Representation => 抽取特征

TF-IDF => 返回给某个文本的“关键词-TFIDF值”的词数对

TF-IDF可以帮我们抽取文本的重要特征，做成item embedding(通过word embedding)

•计算item之间的相似度矩阵

•对于指定的item，选择相似度最大的Top-K个进行输出

Embedding的理解：

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726105817664.png" alt="image-20210726105817664" style="zoom:100%;" />

Embedding指某个对象 X 被嵌入到另外一个对象Y中，映射 f : X → Y

一种降维方式，转换为维度相同的向量

**矩阵分解中的User矩阵，第i行可以看成是第i个user的Embedding。Item矩阵中的第j列可以看成是对第j个Item的Embedding**

Word2Vec工具的使用：

•Word Embedding就是将Word嵌入到一个数学空间里，Word2vec，就是词嵌入的一种

•可以将sentence中的word转换为固定大小的向量表达（Vector Respresentations），

•**其中意义相近的词将被映射到向量空间中相近的位置。**

**•将待解决的问题转换成为单词word和文章doc的对应关系**

大V推荐中，大V => 单词，将每一个用户关注大V的顺序 => 文章

即如果将每个大V的名字看成一个单词，那么在某个用户的关注列表中，所有关注的大V名字可组成一个sentence，Word2Vec对sentence进行学习后，模型就可以用来做相似内容的推荐。

商品推荐中，商品 => 单词，用户对商品的行为顺序 => 文章，方法同上面大V推荐的方法一致。