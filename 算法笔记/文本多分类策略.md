## 文本多分类策略

文本多分类策略：老式的词袋法（tf-idf），著名的词嵌入法（Word2Vec）和最先进的语言模型（BERT）

### 词袋法

#### CountVectorizer

•将文本中的词语转换为词频矩阵

•fit_transform：计算各个词语出现的次数

•get_feature_names：可获得所有文本的关键词

•toarray()：查看词频矩阵的结果。

```python
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    print('feature names:')
    print(vec.get_feature_names())
    print('bag of words:')
    print(bag_of_words.toarray())

```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726095546266.png" alt="image-20210726095546266" style="zoom:100%;" />

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726095601377.png" alt="image-20210726095601377" style="zoom:100%;" />

示例：

以西雅图酒店数据集为例，说明如何利用`CountVectorizer(ngram_range=(n, n)')`手动计算词频最高的20个单词。

```python
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
df = pd.read_csv('Seattle_Hotels.csv', encoding="latin-1")
# 得到酒店描述中n-gram特征中的TopK个
def get_top_n_words(corpus, n=1, k=None):
    # 统计ngram词频矩阵
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    #简单的理解，axis=1按行的方向相加，返回每个行的值；axis=0按列相加，返回每个列的值。
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    # 按照词频从大到小排序
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:k]
common_words = get_top_n_words(df['desc'], 1, 20)
df1 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
df1.groupby('desc').sum()['count'].sort_values().plot(kind='barh', title='去掉停用词后，酒店描述中的Top20单词')
plt.show()

```

unigram

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726094615891.png" alt="image-20210726094615891" style="zoom:100%;" />

\# Bi-Gram

common_words = get_top_n_words(df['desc'], 2, 20)

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726094806097.png" alt="image-20210726094806097" style="zoom:100%;" />



\# Tri-Gram

common_words = get_top_n_words(df['desc'], 3, 20)

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726094812200.png" alt="image-20210726094812200" style="zoom:100%;" />

#### TF-IDF

TF-IDF = TF*IDF,采用一种统计方法，根据字词在文本中出现的次数和在整个语料中出现的文档频率来计算一个字词在整个语料中的重要程度。

•TF：Term Frequency，词频<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726095647451.png" alt="image-20210726095647451" style="zoom:100%;" />

一个单词的重要性和它在文档中出现的次数呈正比。

•IDF：Inverse Document Frequency，逆向文档频率

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726095703692.png" alt="image-20210726095703692" style="zoom:100%;" />

一个单词在文档中的区分度。这个单词出现的文档数越少，区分度越大，IDF越大

TfidfVectorizer:

•将文档集合转化为tf-idf特征值的矩阵

构造函数

•analyzer：word或者char，即定义**特征为词（word）或n-gram字符**

•ngram_range: 参数为二元组(min_n, max_n)，即要提取的n-gram的下限和上限范围

•**max_df**：最大词频，数值为小数[0.0, 1.0],或者是整数，默认为1.0

•**min_df**：最小词频，数值为小数[0.0, 1.0],或者是整数，默认为1.0

•stop_words：停用词，数据类型为列表

功能函数：

fit_transform：进行tf-idf训练，学习到一个字典，并返回Document-term的矩阵，也就是词典中的词在该文档中出现的频次

```python
# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print(tfidf_matrix)
print(tfidf_matrix.shape)

```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726100238980.png" alt="image-20210726100238980" style="zoom:100%;" />

### Word Embedding

#### Embedding

**一种降维方式，将不同特征转换为维度相同的向量**

离线变量转换成one-hot => 维度非常高，可以将它转换为固定size的embedding向量

任何物体，都可以将它转换成为向量的形式，从Trait #1到 #N

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726101653761.png" alt="image-20210726101653761" style="zoom:100%;" />

向量之间，可以使用相似度进行计算

当我们进行推荐的时候，可以选择相似度最大的

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726101808211.png" alt="image-20210726101808211" style="zoom:100%;" />

因此，可以总结出Embedding的特点：

1.固定维度的向量

2.可比较

将Word进行Embedding：

如果我们将King这个单词，通过维基百科的学习，进行GloVe向量化，可以表示成

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726101945128.png" alt="image-20210726101945128" style="zoom:100%;" />

•这50维度的权重大小在[-2,2]，按照颜色的方式来表示

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726101953606.png" alt="image-20210726101953606" style="zoom:100%;" />

我们将King与其他单词进行比较

同样有了向量，我们还可以进行运算，可以看到king-man+woman与queen的相似度最高

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726102045283.png" alt="image-20210726102045283" style="zoom:100%;" />

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726102112558.png" alt="image-20210726102112558" style="zoom:100%;" />

#### Word2Vec

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726102445070.png" alt="image-20210726102445070" style="zoom:100%;" />

•通过Embedding，把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近。

•Word Embedding => 学习隐藏层的权重矩阵

•输入层是one-hot编码

•**隐藏层的神经元数量为hidden_size（Embedding Size）**

•对于输入层和隐藏层之间的权值矩阵W，大小为[vocab_size, hidden_size]

**•输出层为[vocab_size]大小的向量，每一个值代表着输出一个词的概率**

对于输入的one-hot编码：

•在矩阵相乘的时候，选取出矩阵中的某一行，而这一行就是输入词语的word2vec表示

•隐含层的节点个数 = 词向量的维数

•隐层的输出是每个输入单词的Word Embedding

•word2vec，实际上就是一个查找表

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726102918703.png" alt="image-20210726102918703" style="zoom:100%;" />

* Word2Vec的两种模式：

Skip-Gram，给定input word预测上下文

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726102942089.png" alt="image-20210726102942089" style="zoom:100%;" />

CBOW（Continuous Bag-of-Words），给定上下文，预测input word（与Skip-Gram相反）

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726102954031.png" alt="image-20210726102954031" style="zoom:100%;" />

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726103002187.png" alt="image-20210726103002187" style="zoom:100%;" />

#### Word2Vec工具

Gensim工具

•pip install gensim

•可以从非结构化文本中，无监督地学习到隐层的主题向量表达

•每一个向量变换的操作都对应着一个主题模型

•支持TF-IDF，LDA, LSA, word2vec等多种主题模型算法

使用方法：

建立词向量模型：word2vec.Word2Vec(sentences)

window,句子中当前单词和被预测单词的最大距离

min_count,需要训练词语的最小出现次数，默认为5

size,向量维度，默认为100

worker,训练使用的线程数，默认为1即不使用多线程

模型保存 model.save(fname)

模型加载 model.load(fname)



数据集：西游记

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726103337042.png" alt="image-20210726103337042" style="zoom:100%;" />

•journey_to_the_west.txt

•计算小说中的人物相似度，比如孙悟空与猪八戒，孙悟空与孙行者

方案步骤：

Step1，使用分词工具进行分词，比如NLTK,JIEBA

Step2，将训练语料转化成一个sentence的迭代器

Step3，使用word2vec进行训练

Step4，计算两个单词的相似度

```python
# 字词分割，对整个文件内容进行字词分割
def segment_lines(file_list,segment_out_dir,stopwords=[]):
    for i,file in enumerate(file_list):
       segment_out_name=os.path.join(segment_out_dir,'segment_{}.txt'.format(i))
        with open(file, 'rb') as f:
            document = f.read()
            document_cut = jieba.cut(document)
            sentence_segment=[]
            for word in document_cut:
                if word not in stopwords:
                    sentence_segment.append(word)
            result = ' '.join(sentence_segment)
            result = result.encode('utf-8')
            with open(segment_out_name, 'wb') as f2:
                f2.write(result)

# 对source中的txt文件进行分词，输出到segment目录中
file_list=files_processing.get_files_list('./source', postfix='*.txt')
segment_lines(file_list, './segment')

```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726105129135.png" alt="image-20210726105129135" style="zoom:100%;" />

```python
# 将Word转换成Vec，然后计算相似度 
from gensim.models import word2vec
import multiprocessing
# 如果目录中有多个文件，可以使用PathLineSentences
sentences = word2vec.PathLineSentences('./segment')
# 设置模型参数，进行训练
model2 = word2vec.Word2Vec(sentences, size=128, window=5, min_count=5, workers=multiprocessing.cpu_count())
# 保存模型
model2.save('./models/word2Vec.model')
print(model2.wv.similarity('孙悟空', '猪八戒'))
print(model2.wv.similarity('孙悟空', '孙行者'))
print(model2.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者']))

```

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210726105236255.png" alt="image-20210726105236255" style="zoom:100%;" />

### 词袋法与词嵌入

*[词袋法](https://en.wikipedia.org/wiki/Bag-of-words_model)*的模型很简单：从文档语料库构建一个词汇表，并计算单词在每个文档中出现的次数。换句话说，词汇表中的每个单词都成为一个特征，文档由具有相同词汇量长度的矢量（一个“词袋”）表示。例如，我们有3个句子，并用这种方法表示它们：<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/5f4ca56383ac3.png" alt="img" style="zoom:100%;" />

特征矩阵的形状：文档数x词汇表长度

可以想象，这种方法将会导致很严重的维度问题：文件越多，词汇表越大，因此特征矩阵将是一个巨大的稀疏矩阵。所以，为了减少维度问题，词袋法模型通常需要先进行重要的预处理（词清除、删除停用词、词干提取/词形还原）。

词频不一定是文本的最佳表示方法。实际上我们会发现，有些常用词在语料库中出现频率很高，但是它们对目标变量的预测能力却很小。为了解决此问题，有一种词袋法的高级变体，它使用词频-逆向文件频率（[Tf-Idf](https://en.wikipedia.org/wiki/Tf–idf)）代替简单的计数。基本上，一个单词的值和它的计数成正比地增加，但是和它在语料库中出现的频率成反比。

[词嵌入（Word Embedding）](https://en.wikipedia.org/wiki/Word_embedding)是将单词表中的词映射为实数向量的特征学习技术的统称。这些向量是根据每个词出现在另一个词之前或之后的概率分布计算出来的。换一种说法，**上下文相同的单词通常会一起出现在语料库中，所以它们在向量空间中也会很接近。**例如，我们以前面例子中的3个句子为例:

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/1596593018640528.png" alt="img" style="zoom:100%;" />

​                           二维向量空间中的词嵌入

在本教程中，我门将使用这类模型的开山怪: Google的[Word2Vec](https://en.wikipedia.org/wiki/Word2vec)（2013）。其他流行的词嵌入模型还有斯坦福大学的[GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning))（2014）和Facebook的[FastText](https://en.wikipedia.org/wiki/FastText)（2016）.

Word2Vec生成一个包含语料库中的每个独特单词的向量空间，通常有几百维, 这样在语料库中拥有共同上下文的单词在向量空间中的位置就会相互靠近。