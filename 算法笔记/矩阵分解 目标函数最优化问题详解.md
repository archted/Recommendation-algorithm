## 目标函数最优化问题

ALS，Alternating Least Squares，交替最小二乘法

SGD，Stochastic Gradient Descent，随机梯度下降

### ALS算法

#### ALS算法介绍

Alternative Least Square, ALS，交替最小二乘法

ALS-WR是alternating-least-squares with weighted-λ -regularization的缩写，加权正则化交替最小二乘法。

ALS-WR论文：

Large-scale Parallel Collaborative Filtering for the Netflix Prize, 2008

http://machinelearning202.pbworks.com/w/file/fetch/60922097/netflix_aaim08(submitted).pdf

Step1，固定Y 优化X

Step2，固定X 优化Y

重复Step1和2，直到X 和Y 收敛。每次固定一个矩阵，优化另一个矩阵，都是最小二乘问题

#### 观测值取值算数平均值的原因

N次试验，每次观测值略有不同，实际观测值应该为多少？

观测值x=(9.8+9.9+9.8+10.2+10.3)/5=10

为什么要用平均数，而不是中位数，或者是几何平均数？

最小二乘法由统计学家道尔顿(F.Gallton)提出

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140138065-1627796985682.png" alt="image-20210724140138065" style="zoom:100%;" />

$y_i$表示观测值样本，$\hat y$表示我们假设的拟合函数

为什么算数平均值为实际值

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140450432-1627796985682.png" alt="image-20210724140450432" style="zoom:100%;" />



导数为0的时候为最小值，因此

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140457127-1627796985682.png" alt="image-20210724140457127" style="zoom:100%;" />



也就是 <img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140505426-1627796985682.png" alt="image-20210724140505426" style="zoom:100%;" />

所以<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140511625-1627796985682.png" alt="image-20210724140511625" style="zoom:100%;" />

#### 对显式评分矩阵进行分解

最小二乘法是一种重要的数据拟合技术

可以应用于线性回归，非线性回归

Step1，固定Y优化X

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140708341-1627796985682.png" alt="image-20210724140708341" style="zoom:100%;" />

将目标函数转化为矩阵表达形式

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140729011-1627796985682.png" alt="image-20210724140729011" style="zoom:100%;" />

其中，<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140819415-1627796985682.png" alt="image-20210724140819415" style="zoom:100%;" />表示用户u 对m个物品的评分，<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724140844470-1627796985682.png" alt="image-20210724140844470" style="zoom:100%;" />，表示m 个物品的向量。

对目标函数$J$关于$x_u$求梯度，并令梯度为零，得

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141019365-1627796985682.png" alt="image-20210724141019365" style="zoom:100%;" />

求解后可得：

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141033759-1627796985682.png" alt="image-20210724141033759" style="zoom:100%;" />

#### 对隐式矩阵进行分解

##### 隐式矩阵表示方法

除了针对显式评分矩阵，ALS还可以对隐式矩阵进行分解：

将评分看成行为的强度，比如浏览次数，阅读时间

当$r_{ui}>0$时，用户u对商品i有行为

当$r_{ui}=0$时，用户u对商品i没有行为

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141347409-1627796985682.png" alt="image-20210724141347409" style="zoom:100%;" />

**pui 称为用户偏好**：

当用户u 对物品i 有过行为，认为用户u 对物品i感兴趣，$p_{ui}=1$

当用户u 对物品i 没有过行为，认为用户u 对物品i 不感兴趣，$p_{ui}=0$



##### 目标函数

引入置信度

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724163316137-1627796985682.png" alt="image-20210724163316137" style="zoom:100%;" />

当rui>0时，cui与rui线性递增

当rui=0时，cui=1，也就是cui最小值为1

目标函数

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141644987-1627796985683.png" alt="image-20210724141644987" style="zoom:100%;" />

其中，<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141724822-1627796985683.png" alt="image-20210724141724822" style="zoom:100%;" />，<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141730525-1627796985683.png" alt="image-20210724141730525" style="zoom:100%;" />，

xu, yi都为k维列向量，k为隐特征的个数

##### 隐式矩阵分解步骤

将目标函数转化为矩阵形式，并进行求导

Step1，固定Y优化X

同理，求解得<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141909899-1627796985683.png" alt="image-20210724141909899" style="zoom:100%;" />

其中，<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724141931112-1627796985683.png" alt="image-20210724141931112" style="zoom:100%;" />，Λu 为用户u 对所有物品的置信度cui 构成的对角阵

Step2，固定X优化Y

同理，求解得<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724142002793-1627796985683.png" alt="image-20210724142002793" style="zoom:100%;" />

其中，<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724142015723-1627796985684.png" alt="image-20210724142015723" style="zoom:100%;" />，Λi 为所有用户对物品i 的偏好的置信度构成的对角矩阵

#### 工具：spark ml库

spark mllib库（spark3.0版本后废弃）

支持常见的算法，包括 分类、回归、聚类和协同过滤

from pyspark.mllib.recommendation import ALS, Rating, MatrixFactorizationModel

spark ml库（官方推荐）

功能更全面更灵活，ml在DataFrame上的抽象级别更高，数据和操作耦合度更低，使用起来像sklearn

Spark安装很多坑，需要慢慢来

ALS 算法Python代码的实现：

https://github.com/tushushu/imylu/blob/master/imylu/recommend/als.py



### SGD方法

基本思路是以随机方式遍历训练集中的数据，并给出每个已知评分的预测评分。用户和物品特征向量的调整就沿着评分误差越来越小的方向迭代进行，直到误差达到要求。所以，SGD不需要遍历所有的样本即可完成特征向量的求解。

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724144024579-1627796985684.png" alt="image-20210724144024579" style="zoom:100%;" />

让变量沿着目标函数负梯度的方向移动，直到移动到极小值点

y值：

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724143838864-1627796985684.png" alt="image-20210724143838864" style="zoom:100%;" />

其中$\theta$为参数，代表权重，n为特征数,用它来模拟y，需要将损失函数最小化

损失函数

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724143931854-1627796985684.png" alt="image-20210724143931854" style="zoom:100%;" />

需要沿着梯度的反方向，也就是采用梯度下降的方式进行参数更新：

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724144046516-1627796985685.png" alt="image-20210724144046516" style="zoom:100%;" />

其中，a表示学习率（步长），

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724144249326-1627796985685.png" alt="image-20210724144249326" style="zoom:100%;" />

<img src="https://cdn.jsdelivr.net/gh/archted/markdown-img@main/img/image-20210724144255201-1627796985685.png" alt="image-20210724144255201" style="zoom:100%;" />

* 三种梯度下降方式

批量梯度下降：在每次更新时用所有样本

<img src="../L4/assets/image-20210724144509411.png" alt="image-20210724144509411" style="zoom:67%;" />

随机梯度下降：每次更新时用1个样本，用1个样本来近似所有的样本

<img src="../L4/assets/image-20210724144517702.png" alt="image-20210724144517702" style="zoom:67%;" />

注意：没有求和符号

mini-batch梯度下降：每次更新时用b个样本，折中方法

<img src="L4/assets/image-20210724144530848.png" alt="image-20210724144530848" style="zoom:67%;" />